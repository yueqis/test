<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VISUALPUZZLES</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: auto;
      padding: 2em;
      background-color: #fdfdfd;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    .figure {
      margin: 2em 0;
    }
    .figure img {
      width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .caption {
      font-size: 0.9em;
      color: #555;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>VISUALPUZZLES</h1>
<p><strong>Decoupling Multimodal Reasoning Evaluation from Domain Knowledge</strong></p>

<p><strong>Authors:</strong> Yueqi Song*, Tianyue Ou*, Yibo Kong†, Zecheng Li†, Graham Neubig, Xiang Yue</p>
<p><strong>Institution:</strong> Carnegie Mellon University</p>
<p><strong>Project Page:</strong> <a href="https://neulab.github.io/VisualPuzzles/">https://neulab.github.io/VisualPuzzles/</a></p>

<h2>Abstract</h2>
<p>
We introduce <strong>VISUALPUZZLES</strong>, a benchmark that evaluates visual reasoning ability independently of domain knowledge. It spans five categories—algorithmic, analogical, deductive, inductive, and spatial—and includes 1,168 questions sourced and curated to minimize factual knowledge reliance.
</p>

<div class="figure">
  <div class="caption">Figure 1: Model accuracy compared to human performance on VISUALPUZZLES.</div>
</div>

<h2>Example Questions</h2>
<p>VISUALPUZZLES covers five reasoning types. Below are sample questions from each category.</p>

<div class="figure">
  <div class="caption">Figure 2: Example instances from each reasoning category.</div>
</div>

<h2>Key Findings</h2>
<ul>
  <li><strong>Human-Machine Gap:</strong> Even the best models fall below the 5th percentile of human performance.</li>
  <li><strong>Reasoning ≠ Knowledge:</strong> VISUALPUZZLES isolates reasoning from domain knowledge, unlike benchmarks like MMMU.</li>
  <li><strong>Size Isn’t Everything:</strong> Larger models don’t always perform better on reasoning-heavy tasks.</li>
</ul>

<h2>Benchmark Design</h2>
<p>
The benchmark is constructed with minimal domain-specific knowledge requirements, and each question is annotated with a reasoning category and a difficulty level (easy, medium, hard).
</p>

<h3>Dataset Statistics</h3>
<ul>
  <li>Algorithmic: 262</li>
  <li>Analogical: 211</li>
  <li>Deductive: 200</li>
  <li>Inductive: 209</li>
  <li>Spatial: 286</li>
</ul>

<h2>Performance Comparison</h2>

<div class="figure">
  <div class="caption">Figure 3: Reasoning vs. Knowledge accuracy by model size.</div>
</div>

<h3>Disentangling Knowledge</h3>
<p>
GPT-4o was prompted to generate knowledge checklists to verify the reasoning independence of VISUALPUZZLES. Results confirm a significantly lower dependence on domain-specific knowledge compared to benchmarks like MMMU.
</p>

<h3>Reasoning Complexity</h3>
<p>
VISUALPUZZLES questions contain more logical reasoning steps on average than those from MMMU, indicating its higher cognitive demand.
</p>

<div class="figure">
  <div class="caption">Figure 4: Accuracy and response length comparison of reasoning models vs. baselines.</div>
</div>

<h2>Reasoning Patterns and Error Analysis</h2>

<div class="figure">
  <div class="caption">Figure 5: Reasoning patterns on VISUALPUZZLES vs. MMMU.</div>
</div>

<div class="figure">
  <div class="caption">Figure 6: Example of branching reasoning on a spatial task.</div>
</div>

<div class="figure">
  <div class="caption">Figure 8: Error type distribution in Claude-3.7-Sonnet-Thinking.</div>
</div>

<h2>Conclusion</h2>
<p>
VISUALPUZZLES offers a fresh lens for evaluating multimodal reasoning. It highlights the gap between domain knowledge and reasoning ability, revealing that current MLLMs still struggle with general-purpose reasoning—especially under limited knowledge conditions. We hope VISUALPUZZLES will guide the next wave of models toward better abstraction and inference capabilities.
</p>

<hr>
<p><small>*Equal Contribution &nbsp;|&nbsp; †Equal Contribution</small></p>

</body>
</html>
